 Излагаются подходы к решению проблемы понимания языка компьютером, предложенные за период 1950-2012 гг. такими авторами, как Алан Тьюринг, Роджер Шенк, Джон Сёрл, Игорь Мельчук и др. Также предложены попытки создания модели художественного текста с помощью фреймовой модели семантики поэтических произведений. 
Впервые проблема понимания человеческого языка компьютером была поставлена в 1950 г. английским ученым Аланом Тьюрингом в статье «Вычислительные машины и разум» (англ. Computing Machinery and Intelligence) (1). 
Чтобы решить, является ли компьютер разумным в человеческом смысле слова, Тьюринг предложил следующий тест: «Человек взаимодействует с одним компьютером и одним человеком. На основании ответов на вопросы он должен определить, с кем он разговаривает: с человеком или компьютерной программой. Задача компьютерной программы – ввести человека в заблуждение, заставив сделать неверный выбор». 
Тьюринг предложил тест, придуманный по аналогии с игрой для вечеринок «Imitation game» – имитационная игра. В этой игре мужчина и женщина направляются в разные комнаты, а гости пытаются различить их, задавая им серию письменных вопросов и читая напечатанные на машинке ответы на них. По правилам игры и мужчина, и женщина пытаются убедить гостей, что все наоборот. Тьюринг предлагает переделать игру следующим образом: "Теперь зададим вопрос, что случится, если в этой игре примет участие машина? Будет ли задающий вопросы ошибаться так же часто, как если бы он играл с мужчиной и женщиной? Эти вопросы заменяют собой исходный «Может ли машина думать?» 
В 1980 г. Джон Сёрл в работе «Сознание, мозг и программы» (3) рассматривает ситуации, когда должным образом запрограммированный компьютер проходил упрощенную версию теста Тьюринга, и все же – тут Сёрл подкрепляет эти выводы очень сильными аргументами – «понимание» как свойство интеллекта полностью отсутствовало. Один из таких примеров базируется на компьютерной программе, разработанной Роджером Шенком и Р. Абельсоном в 1977 г. (2). Задачей программы была имитация понимания простых историй типа: «Мужчина вошел в ресторан и заказал гамбургер. Когда гамбургер принесли, оказалось, что он сильно подгорел, и рассерженный мужчина выскочил из ресторана, не заплатив по счету и не оставив чаевых». 
В качестве второго примера можно взять другую историю: «Мужчина вошел в ресторан и заказал гамбургер. И, покидая ресторан, он дал официанту щедрые чаевые перед тем, как заплатить по счету». Чтобы проверить «понимание» этих историй компьютером, его «попросили» определить, съел ли мужчина гамбургер в каждом отдельном случае (факт, который не был упомянут в тексте явным образом). На этот простой вопрос к таким простым историям компьютер может дать ответ, совершенно неотличимый от того, что дал бы англоговорящий человек, а именно: «нет» в первом случае и «да» – во втором. Так что в этом, очень узком, смысле машина уже прошла тест Тьюринга! На это, в частности, указывает Р. Пенроуз в своей книге (6), где он обсуждает работы Шенка и Сёрла. Он пишет: «Вопрос, к которому следует обратиться, будет таким: действительно ли подобный положительный результат указывает на истинное понимание, демонстрируемое компьютером – или, возможно, заложенной в него программы? Как аргумент в пользу отрицательного ответа на этот вопрос, Сёрл предлагает свою концепцию «китайской комнаты». Он сразу же оговаривает, что истории должны рассказываться на китайском, а не на английском языке — совершенно несущественная замена – и что все команды для компьютерного алгоритма в этом конкретном случае должны быть представлены набором (английских) инструкций для работы со счетами, на которые нанесены китайские символы. Проводя мысленный эксперимент, Сёрл представлял, что он сам выполняет все манипуляции внутри запертой комнаты. Последовательность символов, описывающая истории, и вопросы к ним подаются в комнату через небольшие прорези. Никакой другой информации извне не допускается. В конце, когда все действия выполнены, последовательность, содержащая ответ, выдается из той же прорези наружу. Поскольку все эти операции есть не что иное, как составляющие процедуры выполнения алгоритма по программе Шенка, то эта последовательность должна содержать просто китайские символы, означающие «да» или «нет» и дающие корректный ответ на вопрос, который как, собственно, и сама история – был изложен по-китайски. При этом Сёрл недву-смысленно дает понять, что он не знает ни слова по-китайски, и посему не имеет ни малейшего представления о содержании рассказанных историй. Тем не менее, выполнив ряд действий, составляющих алгоритм Шенка (инструкции к которому были даны ему на английском языке), он справился бы с задачей не хуже китайца, способного без труда понять эти истории. Довод Сёрла – и весьма сильный, по моему мнению, – заключается в том, что простое выполнение подходящего алгоритма еще не говорит о понимании.
